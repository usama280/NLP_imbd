{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "\n",
    "import nlp\n",
    "import transformers\n",
    "import numpy as np\n",
    "#import IPython; IPython.embed(); exit(1)\n",
    "# import wandb\n",
    "# wandb.init(project=\"CodeBert\", entity=\"usama280\")\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBSentiClassifier(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "        #Change dat\n",
    "    def prepare_data(self):\n",
    "        tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        def _tokenize(x):\n",
    "            #contains both text and encoded values\n",
    "            x['input_ids'] = tokenizer.encode(\n",
    "                    x['question'], \n",
    "                    max_length=32, \n",
    "                    pad_to_max_length=True)\n",
    "            \n",
    "            x['code_ids'] = tokenizer.encode(\n",
    "                    x['answer'], \n",
    "                    max_length=32,\n",
    "                    pad_to_max_length=True)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        def _prepare_ds(folder):\n",
    "#             ds = nlp.load_dataset('neural_code_search',\"evaluation_dataset\", split=f'{folder}[:5%]')\n",
    "            ds = load_dataset(\"neural_code_search\", \"evaluation_dataset\")\n",
    "            ds = ds.map(_tokenize)\n",
    "            ds.set_format(type='torch', columns=['input_ids', 'code_ids'])\n",
    "            \n",
    "            return ds\n",
    "        \n",
    "        \n",
    "        #self.train_ds['train']['question'][0]\n",
    "        #self.train_ds['train']['answer'][0]\n",
    "        self.train_ds, self.test_ds = map(_prepare_ds, ('train', 'test'))\n",
    "        self.train_ds,self.test_ds = self.train_ds['train'],self.test_ds['train']\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, code_ids):\n",
    "#         mask = (input_ids != 0).float()\n",
    "#         logits = self.model(input_ids)\n",
    "        code_vec = self.model(code_ids)\n",
    "        nl_vec = self.model(input_ids)\n",
    "        return code_vec[0],nl_vec[0]\n",
    "    \n",
    "    \n",
    "    #Change\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        code_vec,nl_vec = self.forward(batch['input_ids'], batch['code_ids'])\n",
    "        \n",
    "        scores=torch.einsum(\"ab,cb->ac\",nl_vec,code_vec)\n",
    "        loss = self.loss(scores, torch.arange(batch['code_ids'].size(0), device=scores.device))\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss':loss, 'log':{'train_loss':loss}}\n",
    "\n",
    "    #Change\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        code_vec,nl_vec = self.forward(batch['input_ids'], batch['code_ids'])\n",
    "        \n",
    "        scores=torch.einsum(\"ab,cb->ac\",nl_vec,code_vec)\n",
    "        loss = self.loss(scores, torch.arange(batch['code_ids'].size(0), device=scores.device)) \n",
    "        #acc = (logits.argmax(-1)==batch['stackoverflow_id']).float()\n",
    "        acc=1\n",
    "        \n",
    "        return {'loss':loss, 'acc':acc}\n",
    "    \n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \n",
    "        loss = sum([o['loss'] for o in outputs])/len(outputs)\n",
    "        acc = sum([o['acc'] for o in outputs], 0)/len(outputs)\n",
    "        out = {'val_loss':loss, 'val_acc':acc}\n",
    "        \n",
    "        print(loss)\n",
    "        self.log('val_loss', loss)\n",
    "        return {**out, 'log':out}#appending dic **  \n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                    self.train_ds,\n",
    "                    batch_size=8,\n",
    "                    drop_last=True,\n",
    "                    shuffle=True\n",
    "                )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "                    self.test_ds,\n",
    "                    batch_size=8,\n",
    "                    drop_last=False,\n",
    "                    shuffle=False\n",
    "                )\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(\n",
    "                    self.parameters(),\n",
    "                    lr=1e-2,\n",
    "                    momentum=.9\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = IMDBSentiClassifier() \n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir='logs',\n",
    "        gpus=(1 if torch.cuda.is_available() else 0),\n",
    "        max_epochs=10,\n",
    "#         logger=pl.loggers.WandbLogger(name='codebert-01', project=\"pytorchlightning\")\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Reusing dataset neural_code_search (/home/local/ADILSTU/unadee2/.cache/huggingface/datasets/neural_code_search/evaluation_dataset/1.1.0/a704b7b979fa1e4914c3ea3e59a16d60d6c359d352ea65d033484360329107bc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54fab4bd46346b7b32e7f6e34233237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/local/ADILSTU/unadee2/.cache/huggingface/datasets/neural_code_search/evaluation_dataset/1.1.0/a704b7b979fa1e4914c3ea3e59a16d60d6c359d352ea65d033484360329107bc/cache-da2cf3ab22db799e.arrow\n",
      "Reusing dataset neural_code_search (/home/local/ADILSTU/unadee2/.cache/huggingface/datasets/neural_code_search/evaluation_dataset/1.1.0/a704b7b979fa1e4914c3ea3e59a16d60d6c359d352ea65d033484360329107bc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6136bb29774b06afd3bee7ad4a8153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/local/ADILSTU/unadee2/.cache/huggingface/datasets/neural_code_search/evaluation_dataset/1.1.0/a704b7b979fa1e4914c3ea3e59a16d60d6c359d352ea65d033484360329107bc/cache-da2cf3ab22db799e.arrow\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 109 M \n",
      "1 | loss  | CrossEntropyLoss              | 0     \n",
      "--------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c1538976784758bc2029c9cc23d7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0807, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/ADILSTU/unadee2/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/local/ADILSTU/unadee2/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/local/ADILSTU/unadee2/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (35) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412bd4be3ead41cbadeccbde3bfa7a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/ADILSTU/unadee2/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:405: LightningDeprecationWarning: One of the returned values {'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  warning_cache.deprecation(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865ab3637f9745358301443d2fe81c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0759, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2025bd76724f0e895b5bdb4b363c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/ADILSTU/unadee2/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1051: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
